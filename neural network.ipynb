{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.1-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/lilimatic/opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /Users/lilimatic/opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.6.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy\n",
    "\n",
    "import shap\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary:logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Implementation of the scikit-learn API for XGBoost classification.\n",
       "See :doc:`/python/sklearn_estimator` for more information.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "    n_estimators : Optional[int]\n",
       "        Number of boosting rounds.\n",
       "\n",
       "    max_depth :  Optional[int]\n",
       "        Maximum tree depth for base learners.\n",
       "    max_leaves :\n",
       "        Maximum number of leaves; 0 indicates no limit.\n",
       "    max_bin :\n",
       "        If using histogram-based algorithm, maximum number of bins per feature\n",
       "    grow_policy :\n",
       "        Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
       "        depth-wise. 1: favor splitting at nodes with highest loss change.\n",
       "    learning_rate : Optional[float]\n",
       "        Boosting learning rate (xgb's \"eta\")\n",
       "    verbosity : Optional[int]\n",
       "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "    objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
       "        Specify the learning task and the corresponding learning objective or\n",
       "        a custom objective function to be used (see note below).\n",
       "    booster: Optional[str]\n",
       "        Specify which booster to use: gbtree, gblinear or dart.\n",
       "    tree_method: Optional[str]\n",
       "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
       "        default, XGBoost will choose the most conservative option available.  It's\n",
       "        recommended to study this option from the parameters document :doc:`tree method\n",
       "        </treemethod>`\n",
       "    n_jobs : Optional[int]\n",
       "        Number of parallel threads used to run xgboost.  When used with other\n",
       "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
       "        parallelize and balance the threads.  Creating thread contention will\n",
       "        significantly slow down both algorithms.\n",
       "    gamma : Optional[float]\n",
       "        (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
       "        leaf node of the tree.\n",
       "    min_child_weight : Optional[float]\n",
       "        Minimum sum of instance weight(hessian) needed in a child.\n",
       "    max_delta_step : Optional[float]\n",
       "        Maximum delta step we allow each tree's weight estimation to be.\n",
       "    subsample : Optional[float]\n",
       "        Subsample ratio of the training instance.\n",
       "    sampling_method :\n",
       "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
       "          - ``uniform``: select random training instances uniformly.\n",
       "          - ``gradient_based`` select random training instances with higher probability\n",
       "            when the gradient and hessian are larger. (cf. CatBoost)\n",
       "    colsample_bytree : Optional[float]\n",
       "        Subsample ratio of columns when constructing each tree.\n",
       "    colsample_bylevel : Optional[float]\n",
       "        Subsample ratio of columns for each level.\n",
       "    colsample_bynode : Optional[float]\n",
       "        Subsample ratio of columns for each split.\n",
       "    reg_alpha : Optional[float]\n",
       "        L1 regularization term on weights (xgb's alpha).\n",
       "    reg_lambda : Optional[float]\n",
       "        L2 regularization term on weights (xgb's lambda).\n",
       "    scale_pos_weight : Optional[float]\n",
       "        Balancing of positive and negative weights.\n",
       "    base_score : Optional[float]\n",
       "        The initial prediction score of all instances, global bias.\n",
       "    random_state : Optional[Union[numpy.random.RandomState, int]]\n",
       "        Random number seed.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           Using gblinear booster with shotgun updater is nondeterministic as\n",
       "           it uses Hogwild algorithm.\n",
       "\n",
       "    missing : float, default np.nan\n",
       "        Value in the data which needs to be present as a missing value.\n",
       "    num_parallel_tree: Optional[int]\n",
       "        Used for boosting random forest.\n",
       "    monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
       "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
       "        for more information.\n",
       "    interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
       "        Constraints for interaction representing permitted interactions.  The\n",
       "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
       "        3, 4]]``, where each inner list is a group of indices of features that are\n",
       "        allowed to interact with each other.  See :doc:`tutorial\n",
       "        </tutorials/feature_interaction_constraint>` for more information\n",
       "    importance_type: Optional[str]\n",
       "        The feature importance type for the feature_importances\\_ property:\n",
       "\n",
       "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
       "          \"total_cover\".\n",
       "        * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
       "          without bias.\n",
       "\n",
       "    device : Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
       "\n",
       "    validate_parameters : Optional[bool]\n",
       "\n",
       "        Give warnings for unknown parameter.\n",
       "\n",
       "    enable_categorical : bool\n",
       "\n",
       "        .. versionadded:: 1.5.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
       "        should be used to specify categorical data type.  Also, JSON/UBJSON\n",
       "        serialization format is required.\n",
       "\n",
       "    feature_types : Optional[FeatureTypes]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        Used for specifying feature types without constructing a dataframe. See\n",
       "        :py:class:`DMatrix` for details.\n",
       "\n",
       "    max_cat_to_onehot : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
       "        for categorical data.  When number of categories is lesser than the threshold\n",
       "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
       "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
       "        categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    max_cat_threshold : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Maximum number of categories considered for each split. Used only by\n",
       "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
       "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    multi_strategy : Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        .. note:: This parameter is working-in-progress.\n",
       "\n",
       "        The strategy used for training multi-target models, including multi-target\n",
       "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
       "        more information.\n",
       "\n",
       "        - ``one_output_per_tree``: One model for each target.\n",
       "        - ``multi_output_tree``:  Use multi-target trees.\n",
       "\n",
       "    eval_metric : Optional[Union[str, List[str], Callable]]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        Metric used for monitoring the training result and early stopping.  It can be a\n",
       "        string or list of strings as names of predefined metric in XGBoost (See\n",
       "        doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
       "        user defined metric that looks like `sklearn.metrics`.\n",
       "\n",
       "        If custom objective is also provided, then custom metric should implement the\n",
       "        corresponding reverse link function.\n",
       "\n",
       "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
       "        object is provided, it's assumed to be a cost function and by default XGBoost will\n",
       "        minimize the result during early stopping.\n",
       "\n",
       "        For advanced usage on Early stopping like directly choosing to maximize instead of\n",
       "        minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
       "        for more.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "             This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old\n",
       "             one receives un-transformed prediction regardless of whether custom\n",
       "             objective is being used.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from sklearn.datasets import load_diabetes\n",
       "            from sklearn.metrics import mean_absolute_error\n",
       "            X, y = load_diabetes(return_X_y=True)\n",
       "            reg = xgb.XGBRegressor(\n",
       "                tree_method=\"hist\",\n",
       "                eval_metric=mean_absolute_error,\n",
       "            )\n",
       "            reg.fit(X, y, eval_set=[(X, y)])\n",
       "\n",
       "    early_stopping_rounds : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        - Activates early stopping. Validation metric needs to improve at least once in\n",
       "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
       "          least one item in **eval_set** in :py:meth:`fit`.\n",
       "\n",
       "        - If early stopping occurs, the model will have two additional attributes:\n",
       "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
       "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
       "          number of trees during inference. If users want to access the full model\n",
       "          (including trees built after early stopping), they can specify the\n",
       "          `iteration_range` in these inference methods. In addition, other utilities\n",
       "          like model plotting can also use the entire model.\n",
       "\n",
       "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
       "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
       "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
       "          metric will be used for early stopping.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "            This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
       "\n",
       "    callbacks : Optional[List[TrainingCallback]]\n",
       "        List of callback functions that are applied at end of each iteration.\n",
       "        It is possible to use predefined callbacks by using\n",
       "        :ref:`Callback API <callback_api>`.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           States in callback are not preserved during training, which means callback\n",
       "           objects can not be reused for multiple training sessions without\n",
       "           reinitialization or deepcopy.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            for params in parameters_grid:\n",
       "                # be sure to (re)initialize the callbacks before each run\n",
       "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
       "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
       "                reg.fit(X, y)\n",
       "\n",
       "    kwargs : dict, optional\n",
       "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
       "        can be found :doc:`here </parameter>`.\n",
       "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
       "        dict simultaneously will result in a TypeError.\n",
       "\n",
       "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
       "            that parameters passed via this argument will interact properly\n",
       "            with scikit-learn.\n",
       "\n",
       "        .. note::  Custom objective function\n",
       "\n",
       "            A custom objective function can be provided for the ``objective``\n",
       "            parameter. In this case, it should have the signature\n",
       "            ``objective(y_true, y_pred) -> grad, hess``:\n",
       "\n",
       "            y_true: array_like of shape [n_samples]\n",
       "                The target values\n",
       "            y_pred: array_like of shape [n_samples]\n",
       "                The predicted values\n",
       "\n",
       "            grad: array_like of shape [n_samples]\n",
       "                The value of the gradient for each sample point.\n",
       "            hess: array_like of shape [n_samples]\n",
       "                The value of the second derivative for each sample point\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     XGBRFClassifier\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XGBClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data \n",
    "\n",
    "hepatitis = fetch_ucirepo(id=46) \n",
    "\n",
    "mask = ~hepatitis.data.features.isna().any(axis=1)\n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = hepatitis.data.features[mask] \n",
    "y = hepatitis.data.targets[mask]\n",
    "\n",
    "df = pd.concat([X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y = torch.tensor(y.values - 1, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wide(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(19, 180)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(180, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(19, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(60, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3781\n",
      "8581\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model1 = Wide()\n",
    "model2 = Deep()\n",
    "print(sum([x.reshape(-1).shape[0] for x in model1.parameters()]))  # 11161\n",
    "print(sum([x.reshape(-1).shape[0] for x in model2.parameters()]))  # 11041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    n_epochs = 250   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (wide): 0.92\n",
      "Accuracy (wide): 0.91\n",
      "Accuracy (wide): 0.91\n",
      "Accuracy (wide): 1.00\n",
      "Accuracy (wide): 0.82\n"
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_wide = []\n",
    "for train, val in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Wide()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[val], y_train[val])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "    cv_scores_wide.append(acc)\n",
    "cv_scores_deep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (deep): 0.83\n",
      "Accuracy (deep): 0.91\n",
      "Accuracy (deep): 0.91\n",
      "Accuracy (deep): 0.82\n",
      "Accuracy (deep): 0.91\n",
      "Wide: 91.06% (+/- 5.76%)\n",
      "Deep: 87.58% (+/- 4.11%)\n"
     ]
    }
   ],
   "source": [
    "for train, val in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[val], y_train[val])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    "\n",
    "# evaluate the model\n",
    "wide_acc = np.mean(cv_scores_wide)\n",
    "wide_std = np.std(cv_scores_wide)\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.    1.    1.    2.    2.    2.    2.    1.    1.    1.    2.    2.\n",
      "   2.    0.7 164.   44.    3.1  41.    1. ] -> [0.52576196] (expected [1.])\n",
      "[52.   1.   1.   1.   2.   2.   2.   2.   2.   2.   2.   2.   2.   0.7\n",
      " 75.  55.   4.  21.   1. ] -> [0.49544206] (expected [1.])\n",
      "[41.   1.   2.   1.   1.   2.   2.   2.   1.   2.   2.   2.   2.   0.9\n",
      " 81.  60.   3.9 52.   1. ] -> [0.78913] (expected [1.])\n",
      "[30.   1.   2.   1.   2.   2.   2.   2.   2.   2.   2.   2.   2.   0.7\n",
      " 52.  38.   3.9 52.   1. ] -> [0.8192824] (expected [1.])\n",
      "[ 20.    1.    1.    2.    1.    1.    1.    2.    2.    2.    1.    1.\n",
      "   2.    1.  160.  118.    2.9  23.    2. ] -> [0.7254746] (expected [1.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n",
    " \n",
    "    # Plot the ROC curve\n",
    "    y_pred = model(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    plt.plot(fpr, tpr) # ROC curve = TPR vs FPR\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X100 = shap.utils.sample(X, len(df.copy().dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-01e72e0e6344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaterfall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_display\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shap_values' is not defined"
     ]
    }
   ],
   "source": [
    "shap.plots.waterfall(shap_values[sample_ind], max_display=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
